{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering Notes (Theory)\n",
    "\n",
    "## 1. Feature Engineering\n",
    "Feature Engineering is the process of transforming raw data into meaningful features that improve model performance.\n",
    "\n",
    "It includes four main steps:\n",
    "- **Feature Transformation**\n",
    "- **Feature Construction**\n",
    "- **Feature Selection**\n",
    "- **Feature Extraction**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Transformation\n",
    "\n",
    "### (a) Missing Value Imputation\n",
    "- Many datasets have missing values.\n",
    "- Replace missing values with:\n",
    "  - **Mean / Median / Mode** → keeps data balanced\n",
    "  - Drop columns → if too many missing values\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Handling Categorical Features\n",
    "- ML models work with numbers, not text.\n",
    "- Convert categorical features using:\n",
    "  - **Label Encoding** → assigns numbers to categories\n",
    "  - **One-Hot Encoding** → creates binary columns for each category\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Outlier Detection\n",
    "- Outliers = extreme values that can mislead the model.\n",
    "- Detection methods:\n",
    "  - **IQR (Interquartile Range)**\n",
    "- Handling methods:\n",
    "  - Remove them\n",
    "  - Cap or transform them\n",
    "\n",
    "---\n",
    "\n",
    "### (d) Feature Scaling\n",
    "- Some algorithms are sensitive to feature magnitude (e.g., Logistic Regression, SVM, KNN).\n",
    "- Methods:\n",
    "  - **Standardization** → mean = 0, std = 1\n",
    "  - **Normalization** → scale values between [0,1]\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Construction\n",
    "Creating new features from existing ones.\n",
    "\n",
    "Examples:\n",
    "- **Family Size** = (SibSp + Parch + 1)\n",
    "- **IsAlone** = 1 if no family, else 0\n",
    "- **Title extraction** from names (Mr, Mrs, Miss, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Selection\n",
    "Not all features are useful. Some may be redundant or noisy.\n",
    "\n",
    "Techniques:\n",
    "- **Statistical tests** (Chi-square, ANOVA)\n",
    "- **Model-based selection** (Random Forest, XGBoost feature importance)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Feature Extraction\n",
    "Reduce dimensionality while keeping important information.\n",
    "\n",
    "- **PCA (Principal Component Analysis)** → converts features into fewer dimensions , LDA , TSNA\n",
    "- Benefits:\n",
    "  - Reduces overfitting\n",
    "  - Helps visualization\n",
    "  - Avoids curse of dimensionality\n"
   ],
   "id": "c4d013c918f3f4f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
