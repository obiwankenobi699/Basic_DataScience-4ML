{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering Notes (Theory)\n",
    "\n",
    "## 1. Feature Engineering\n",
    "Feature Engineering is the process of transforming raw data into meaningful features that improve model performance.\n",
    "\n",
    "It includes four main steps:\n",
    "- **Feature Transformation**\n",
    "- **Feature Construction**\n",
    "- **Feature Selection**\n",
    "- **Feature Extraction**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Transformation\n",
    "\n",
    "### (a) Missing Value Imputation\n",
    "- Many datasets have missing values.\n",
    "- Replace missing values with:\n",
    "  - **Mean / Median / Mode** ‚Üí keeps data balanced\n",
    "  - Drop columns ‚Üí if too many missing values\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Handling Categorical Features\n",
    "- ML models work with numbers, not text.\n",
    "- Convert categorical features using:\n",
    "  - **Label Encoding** ‚Üí assigns numbers to categories\n",
    "  - **One-Hot Encoding** ‚Üí creates binary columns for each category\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Outlier Detection\n",
    "- Outliers = extreme values that can mislead the model.\n",
    "- Detection methods:\n",
    "  - **IQR (Interquartile Range)**\n",
    "- Handling methods:\n",
    "  - Remove them\n",
    "  - Cap or transform them\n",
    "\n",
    "---\n",
    "\n",
    "### (d) Feature Scaling\n",
    "- Some algorithms are sensitive to feature magnitude (e.g., Logistic Regression, SVM, KNN).\n",
    "- Methods:\n",
    "  - **Standardization** ‚Üí mean = 0, std = 1\n",
    "  - **Normalization** ‚Üí scale values between [0,1]\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Construction\n",
    "Creating new features from existing ones.\n",
    "\n",
    "Examples:\n",
    "- **Family Size** = (SibSp + Parch + 1)\n",
    "- **IsAlone** = 1 if no family, else 0\n",
    "- **Title extraction** from names (Mr, Mrs, Miss, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Selection\n",
    "Not all features are useful. Some may be redundant or noisy.\n",
    "\n",
    "Techniques:\n",
    "- **Statistical tests** (Chi-square, ANOVA)\n",
    "- **Model-based selection** (Random Forest, XGBoost feature importance)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Feature Extraction\n",
    "Reduce dimensionality while keeping important information.\n",
    "\n",
    "- **PCA (Principal Component Analysis)** ‚Üí converts features into fewer dimensions , LDA , TSNA\n",
    "- Benefits:\n",
    "  - Reduces overfitting\n",
    "  - Helps visualization\n",
    "  - Avoids curse of dimensionality\n"
   ],
   "id": "c4d013c918f3f4f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# üìè Feature Scaling ‚Äì Theory & Practice\n",
    "\n",
    "Feature scaling is a **core step in feature engineering**.\n",
    "It is used to **standardize the range of independent variables** (features) in a dataset.\n",
    "Many ML algorithms are **sensitive to the scale of features**, especially:\n",
    "\n",
    "- Distance-based algorithms (KNN, K-Means, SVM)\n",
    "- Gradient-based optimization algorithms (Linear/Logistic Regression, Neural Networks)\n",
    "\n",
    "Scaling ensures that **all features contribute equally** to the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Importance of Feature Scaling\n",
    "\n",
    "1. **Distance-based algorithms**\n",
    "   - Example: Euclidean distance in KNN:\n",
    "     \\[\n",
    "     \\text{distance} = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ...}\n",
    "     \\]\n",
    "     If features have different scales, larger magnitude features dominate the distance.\n",
    "\n",
    "2. **Faster convergence of gradient descent**\n",
    "   - Similar scale features help optimization algorithms reach minima efficiently.\n",
    "\n",
    "3. **Better regularization**\n",
    "   - Regularization techniques (L1/L2) assume features are on similar scales.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Feature Scaling Techniques\n",
    "\n",
    "### 2.1 Normalization (Min-Max Scaling)\n",
    "\n",
    "- **Goal:** Scale features to a **fixed range**, usually `[0,1]`.\n",
    "- **Formula:**\n",
    "\\[\n",
    "X_{\\text{normalized}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "\\]\n",
    "\n",
    "- **Pros:** Preserves relationships and distribution shape.\n",
    "- **Cons:** Sensitive to outliers.\n",
    "\n",
    "**Python Example:**\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "````\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Subtract the **minimum value** of the feature.\n",
    "* Divide by the **range (max - min)**.\n",
    "* Resulting values lie between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Standardization (Z-score Scaling)\n",
    "\n",
    "* **Goal:** Center features at **mean = 0** and scale to **unit variance**.\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mu$ = mean of the feature\n",
    "\n",
    "* $\\sigma$ = standard deviation\n",
    "\n",
    "* **Pros:** Less sensitive to outliers than normalization.\n",
    "\n",
    "* **Commonly used in:** SVM, Logistic Regression, PCA, Neural Networks.\n",
    "\n",
    "**Python Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Subtract **mean** ($\\mu$) ‚Üí center at 0\n",
    "* Divide by **standard deviation** ($\\sigma$) ‚Üí unit variance\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Other Scaling Techniques\n",
    "\n",
    "| Technique       | Formula                                                    | When to Use            |    |                                 |\n",
    "| --------------- | ---------------------------------------------------------- | ---------------------- | -- | ------------------------------- |\n",
    "| Robust Scaling  | $X_{\\text{robust}} = \\frac{X - \\text{median}}{\\text{IQR}}$ | Data with **outliers** |    |                                 |\n",
    "| Max Abs Scaling | (X\\_{\\text{maxabs}} = \\frac{X}{                            | X\\_{\\max}              | }) | Sparse data (e.g., text/TF-IDF) |\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Normalization vs Standardization\n",
    "\n",
    "| Aspect   | Normalization                                                | Standardization                           |\n",
    "| -------- | ------------------------------------------------------------ | ----------------------------------------- |\n",
    "| Range    | \\[0,1]                                                       | Mean = 0, Variance = 1                    |\n",
    "| Formula  | $X_{\\text{norm}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}$ | $X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}$ |\n",
    "| Outliers | Sensitive                                                    | Less sensitive                            |\n",
    "| Use Case | Neural Networks, when range matters                          | SVM, Logistic Regression, PCA             |\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Best Practices\n",
    "\n",
    "* **Fit on training data, transform test data** to avoid data leakage:\n",
    "\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Do NOT fit again\n",
    "```\n",
    "\n",
    "* Choose **scaling method** based on:\n",
    "\n",
    "  * Algorithm type\n",
    "  * Data distribution\n",
    "  * Presence of outliers\n",
    "\n",
    "\n"
   ],
   "id": "96727bfe72c371d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T04:59:12.886540Z",
     "start_time": "2025-09-22T04:59:12.879416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "7f0454b60be39ac",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T04:59:14.951058Z",
     "start_time": "2025-09-22T04:59:14.913701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(r'C:\\Basic_Datascience_4ML\\assets\\data\\Social_Network_Ads.csv')\n",
    "df.head()"
   ],
   "id": "bd8d0d9e0177c1c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Age  EstimatedSalary  Purchased\n",
       "0   19            19000          0\n",
       "1   35            20000          0\n",
       "2   26            43000          0\n",
       "3   27            57000          0\n",
       "4   19            76000          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üé≤ What does `random_state` do?\n",
    "\n",
    "- `random_state` controls the **random number generator** used in functions like `train_test_split`.\n",
    "- Ensures **reproducibility** of results:\n",
    "  - Same `random_state` ‚Üí same split every run.\n",
    "  - Different (or no) `random_state` ‚Üí different splits on each run.\n",
    "\n",
    "### üîπ Why it's useful?\n",
    "- **Reproducibility** ‚Üí others can reproduce your results.\n",
    "- **Debugging** ‚Üí consistent behavior for testing.\n",
    "- **Fair comparison** ‚Üí same train/test split for multiple models.\n",
    "\n",
    "### üîπ Example\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(10).reshape((5, 2))\n",
    "y = np.array([1, 0, 1, 0, 1])\n",
    "\n",
    "# Fixed random_state = reproducible split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Data:\\n\", X_train)\n",
    "print(\"Testing Data:\\n\", X_test)\n"
   ],
   "id": "92ab744fa4be88b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T05:18:16.442801Z",
     "start_time": "2025-09-22T05:18:15.982704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop('Purchased', axis=1),\n",
    "    df['Purchased'],\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "print(y_test)\n"
   ],
   "id": "4edd82102c0b779d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132    0\n",
      "309    0\n",
      "341    0\n",
      "196    0\n",
      "246    0\n",
      "      ..\n",
      "216    0\n",
      "259    1\n",
      "49     0\n",
      "238    0\n",
      "343    1\n",
      "Name: Purchased, Length: 120, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìò Standardization with `StandardScaler`\n",
    "\n",
    "The code uses **`StandardScaler`** from scikit-learn to standardize features.\n",
    "Standardization means **scaling features** so that they have:\n",
    "\n",
    "- Mean = **0**\n",
    "- Standard Deviation = **1**\n",
    "\n",
    "This helps ML algorithms (like Logistic Regression, SVM, Neural Networks, etc.) perform better because features are on the same scale.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Code Breakdown\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 1. Fit only on training data ‚Üí learn mean & std for each feature\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# 2. Transform training and test data using training stats\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "id": "c4b638a90871fd18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training set to learn mean and std\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform train and test sets using parameters learned from train set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# it gives me numpy array but i want dataframe so i have to convert it to data frame\n",
    "print(X_train_scaled)\n",
    "\n",
    "\n"
   ],
   "id": "eecdf9e24af7cae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìò Rounding `describe()` output\n",
    "\n",
    "- `X_test_scaled.describe()` ‚Üí shows full precision descriptive statistics (mean, std, min, max, quartiles).\n",
    "- `np.round(X_test_scaled.describe(), 1)` ‚Üí rounds all statistics to **1 decimal place** for easier readability.\n",
    "\n",
    "### üîπ Key Points\n",
    "- Both outputs represent the **same data**; only the **display format differs**.\n",
    "- Rounding does **not change the underlying data**, just the presentation.\n",
    "\n",
    "### üîπ Example\n",
    "```python\n",
    "X_test_scaled.describe()        # Full precision\n",
    "np.round(X_test_scaled.describe(), 1)  # Rounded to 1 decimal\n"
   ],
   "id": "4916ecc19ecd9394"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "X_test_scaled.describe()\n",
    "np,round(X_test_scaled.describe(),1)"
   ],
   "id": "289079211c97a292",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_test_scaled.describe()",
   "id": "fd514bd03bcc63c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d3ae4bac5a9d8c6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, subplot 1\n",
    "sns.scatterplot(data=X_test_scaled, x='Age', y='EstimatedSalary',color='red')\n",
    "plt.title(\"Scaled Test Data\")\n",
    "\n",
    "plt.subplot(1, 2, 2)  # subplot 2\n",
    "sns.scatterplot(data=X_test, x='Age', y='EstimatedSalary')\n",
    "plt.title(\"Original Test Data\")\n",
    "\n",
    "plt.tight_layout()   # Adjust layout to prevent overlap\n",
    "plt.show()\n"
   ],
   "id": "f76c562338f58007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "so here in Scalled graph the poin of 0 is in center",
   "id": "1e67e63933d55a24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.kdeplot(data=X_test_scaled, color='red')\n",
    "plt.title(\"Scaled Test Data\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.kdeplot(data=X_test, color='blue')\n",
    "plt.title(\"Original Test Data\")"
   ],
   "id": "62f20976e56d0115",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìò Feature Scaling & Standardization\n",
    "\n",
    "### üîπ Outliers and Scaling\n",
    "- **Outliers** can distort some scaling methods (especially Min-Max scaling).\n",
    "- Standardization (`StandardScaler`) is **less sensitive to outliers** than Min-Max, but extreme outliers can still affect mean & std.\n",
    "- ‚úÖ It's recommended to **handle or remove outliers** before scaling for better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ When to use Standardization?\n",
    "- Standardization **centers data to mean=0 and std=1**, making features comparable.\n",
    "- Useful when algorithms rely on **distance or gradient calculations**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Algorithms where Standardization is important\n",
    "1. **K-Means** ‚Üí distance-based, scales affect cluster formation.\n",
    "2. **K-Nearest Neighbors (KNN)** ‚Üí distance metric depends on feature scale.\n",
    "3. **PCA (Principal Component Analysis)** ‚Üí maximizes variance; unscaled features dominate.\n",
    "4. **Artificial Neural Networks** ‚Üí faster convergence, stable gradients.\n",
    "5. **Gradient Descent based models** ‚Üí improves learning speed and convergence.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Conclusion:**\n",
    "- Standardization is generally **beneficial** and has **no negative impact** on most algorithms.\n",
    "- Always consider handling **outliers first** for best results.\n"
   ],
   "id": "96aa1b1ddcc372ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
